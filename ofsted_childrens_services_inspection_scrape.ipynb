{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ofsted Inspection Reports Scrape Tool \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary:</b><br>\n",
    "<span style=\"font-size:10pt\">Scrapes local authorities 'Childrens Services Ofsted Inspection' reports from https://reports.ofsted.gov.uk/ in pdf format. The Children's Services Inspection reports are downloaded locally as their original pdf, organised into folders by local authority name and their URN. The LA naming is cleaned to better reflect a more standard/onward process use of LA naming - e.g. 'Nottingham City Council' becomes 'Nottingham' and 'Barnsley Metropolitan Borough Council' becomes 'Barnsley'. </span><br>\n",
    "    <b>Folder naming convention:</b><br>\n",
    "        <span style=\"font-size:10pt\">\\provider_urn+local_authority_name(lowercase)...pdf inspection files</span><br>\n",
    "<br>\n",
    "<b>N.B/Pre-requisites:</b><br>\n",
    "<span style=\"font-size:10pt\">Relies on Ofsted's continued use of nonvisual css element descriptors on the web site. Obv not ideal to rely on anything in the web-space, but any scrape process, however robust, is undermined/dictated by subsequent page changes. The tool has avoided the use of Selenium or similar as this is more likely to be impacted by visual design changes on the page(s). Instead it relies on the underlying php search process, and associated php generated links.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Backlog/to-do:</b><br>\n",
    "\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>Improve the loop around the pdf scrape process to skip over once most recent obtained.</li>\n",
    "    <li>Scrape all/more granular inspection grades not just overall effectiveness.</li>\n",
    "    <li>Be able to set a year threshold for reports to ignore historic (esp relevant to previous due to formatting issues on early reports.)</li>\n",
    "    <li>Obtain/auto-lookup region codes so these can be added to LA summary export file with LA name.</li>\n",
    "    <li>Are old LA numbers still being used? Add those next to URN.</li>\n",
    "    <li>Local folder link in excel export to enable direct link from summary report to saved pdf files.</li>\n",
    "    <li>...</li>\n",
    "    <li>...</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export options\n",
    "\n",
    "export_summary_filename = 'ofsted_tracker_update'\n",
    "export_file_type         = 'csv' # Excel / csv currently supported\n",
    "\n",
    "\n",
    "# scrape inspection grade/data from pdf reports\n",
    "pdf_data_capture = True # True is default (scrape within pdf inspection reports)\n",
    "                        # This impacts run time E.g False == ~1m20 / True == ~ 4m10\n",
    "                        # False == only pdfs/list of LA's+link to most recent exported. Not grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ofsted site/page admin settings\n",
    "\n",
    "max_page_results = 200 # Set max number of search results to show on page(MUST be > total number of LA's!) \n",
    "url_stem = 'https://reports.ofsted.gov.uk/'\n",
    "search_url = 'search?q=&location=&lat=&lon=&radius=&level_1_types=3&level_2_types%5B%5D=12' # On to-do list\n",
    "max_page_results_url = '&rows=' + str(max_page_results) # Coerce results page to display ALL providers on single results page without next/pagination\n",
    "\n",
    "# resultant complete url to process\n",
    "url = url_stem + search_url + max_page_results_url\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Script admin settings\n",
    "\n",
    "# Keep warnings quiet unless priority\n",
    "import logging\n",
    "import subprocess\n",
    "logging.getLogger('org.apache.pdfbox').setLevel(logging.ERROR)\n",
    "subprocess.call(['my_command'], stderr=subprocess.DEVNULL)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non-standard modules that might need installing\n",
    "# !pip install PyPDF2\n",
    "# !pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# pdf search/data extraction\n",
    "import io\n",
    "import tabula   \n",
    "import PyPDF2   \n",
    "import re       \n",
    "\n",
    "# used in handling inspection dates\n",
    "from dateutil import parser \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function defs\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given a URL, returns a BeautifulSoup object.\n",
    "    Args: url (str): The URL to fetch and parse.\n",
    "    Returns: BeautifulSoup: The parsed HTML content.\n",
    "    \"\"\"\n",
    "    timeout_seconds = 10 # lets not assume the Ofsted page is up\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout_seconds)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except RequestException as e:\n",
    "        print(f\"An error occurred while fetching the URL '{url}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_provider_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the name according to the logic provided.\n",
    "    Args:\n",
    "        name (str): The original name to be cleaned.\n",
    "    Returns:\n",
    "        str: The cleaned name.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    name = name.lower().replace('  ', ' ')\n",
    "    \n",
    "    # Remove specific phrases\n",
    "    name = name.replace(\"city of \", \"\").replace(\"metropolitan district council\", \"\").replace(\"london borough of\", \"\").replace(\"council of\", \"\")\n",
    "    \n",
    "    # Remove undesired words and join the remaining parts\n",
    "    name_parts = [part for part in name.split() if part not in ['city', 'metropolitan', 'borough', 'council', 'county', 'district', 'the']]\n",
    "    return ' '.join(name_parts)\n",
    "\n",
    "\n",
    "\n",
    "def extract_inspection_data(pdf_content):\n",
    "    \"\"\"\n",
    "    Extracts the inspector's name, overall Ofsted grade, and inspection dates from the first page of a PDF report.\n",
    "\n",
    "    Args:\n",
    "        pdf_content (bytes): The content of the PDF file as bytes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the inspector's name, overall Ofsted grade, and inspection dates, or None if not found.\n",
    "\n",
    "    Notes:\n",
    "        This function extracts information from the first page of the PDF report. The inspector's name is extracted using a\n",
    "        regular expression search for the string \"Lead inspector:\". The overall Ofsted grade is extracted from a table that\n",
    "        appears on the first page of the report. The function uses the tabula library to extract the table data. The inspection\n",
    "        dates are also extracted using a regular expression search for the string \"Inspection dates:\". The function attempts to\n",
    "        parse the inspection dates into datetime objects and format them as \"dd/mm/yyyy\". The final output is a dictionary\n",
    "        containing the extracted information or None if any of the information could not be found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when an unknown grade value is found during grade extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a file-like buffer for the PDF content\n",
    "    with io.BytesIO(pdf_content) as buffer:\n",
    "        # Read the PDF content for text extraction\n",
    "        reader = PyPDF2.PdfReader(buffer)\n",
    "        page = reader.pages[0]\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Find the inspector's name using a regular expression\n",
    "        match = re.search(r\"Lead inspector:\\s*(.+)\", text)\n",
    "        if match:\n",
    "            inspector_name = match.group(1)\n",
    "            \n",
    "            inspector_name = inspector_name.split(',')[0].strip()       # Remove everything after the first comma\n",
    "            inspector_name = inspector_name.replace(\"HMI\", \"\").rstrip() # Remove \"HMI\" and any trailing spaces\n",
    "\n",
    "        else:\n",
    "            inspector_name = None\n",
    "\n",
    "        # Read the PDF and extract the table on the first page\n",
    "        try:\n",
    "            buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "            tables = tabula.read_pdf(buffer, pages=1, multiple_tables=True)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the PDF: {e}\")\n",
    "            tables = []\n",
    "\n",
    "    # Find the 'Overall effectiveness' row and extract the corresponding grade\n",
    "    inspection_grade = None\n",
    "    for table in tables:\n",
    "        if 'Judgement' in table.columns and 'Grade' in table.columns:\n",
    "            for index, row in table.iterrows():\n",
    "                if row['Judgement'] == 'Overall effectiveness':\n",
    "\n",
    "                    try:\n",
    "                        inspection_grade = str(row['Grade']) # Convert the grade to a string (previously float values found in some reports)\n",
    "\n",
    "                        if \"requires improvement\" in inspection_grade.lower():\n",
    "                            # Some RI texts come with extra wording/baggage. Overwrite with clean val to aid clean-up\n",
    "                            inspection_grade = \"Requires improvement\"\n",
    "\n",
    "                    except Exception as e:\n",
    "                        inspection_grade = \"Unknown value type : \" + inspection_grade\n",
    "\n",
    "                        error_msg = f\"unknown value found: \\\"unknown : {inspection_grade}\\\"\"\n",
    "                        raise ValueError(error_msg)\n",
    "                        \n",
    "                    break\n",
    "                \n",
    "            if inspection_grade is not None:\n",
    "                break\n",
    "\n",
    "\n",
    "    # Find the inspection dates using a regular expression\n",
    "    date_match = re.search(r\"Inspection dates:\\s*(.+)\", text)\n",
    "\n",
    "    if date_match:\n",
    "        # IF there was date data\n",
    "\n",
    "\n",
    "        inspection_dates = date_match.group(1).strip()\n",
    "            \n",
    "        # Some initial clean up based on historic data obs\n",
    "        inspection_dates = inspection_dates.replace(\".\", \"\")\n",
    "        inspection_dates = inspection_dates.replace(\"\\u00A0\", \" \") # Remove non-breaking space (Seen in nottingham report)\n",
    "        inspection_dates = re.sub(r\"[\\u2012\\u2013\\u2014\\u2212\\-]+\", \" to \", inspection_dates) # replace en dash char (\"\\u2013\"), em dash (\"\\u2014\"), or (\"-\") \n",
    "        inspection_dates = inspection_dates.split(\"and\")[0].strip() # Need this because we have such as :\n",
    "                                                                    # \"8 July 2019 to 12 July 2019 and 7 August 2019 to 8 August 2019\"\n",
    "                                                                    # E.g. Derbyshire\n",
    "        inspection_dates = re.sub(r'(\\d)\\s(\\d)', r'\\1\\2', inspection_dates) # Fix white spaces between date numbers e.g. \"wiltshire,\t1 9 June 2019\"\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(inspection_dates, str):\n",
    "            # data was as expected\n",
    "            year_match = re.search(r\"\\d{4}\", inspection_dates)\n",
    "            if year_match:\n",
    "                year = year_match.group(0) # get single copy of yyyy\n",
    "\n",
    "                # Now remove the year from the inspection_dates string\n",
    "                inspection_dates_cleaned = inspection_dates.replace(year, \"\").strip()\n",
    "\n",
    "            else:\n",
    "                # We had inspection_dates data but no recognisable year\n",
    "                year = None\n",
    "                inspection_dates_cleaned = inspection_dates\n",
    "\n",
    "        else:\n",
    "            # spurious data\n",
    "            # inspection_dates arrived with non-str, set default val\n",
    "            print(\"Error: inspection_dates is not a string. Type is\", type(inspection_dates))\n",
    "            inspection_dates_cleaned = None \n",
    "\n",
    "\n",
    "        # Now that we have already removed/cleaned those with 'and .....'\n",
    "        # Split the inspection_dates_cleaned string using ' to ' as the delimiter and limit the number of splits to 1\n",
    "        date_parts = inspection_dates_cleaned.split(' to ', maxsplit=1) # expect only 1 instance of 'to' between date vals\n",
    "        \n",
    "\n",
    "  \n",
    "        # Get the seperate inspection date(s) \n",
    "        start_date = date_parts[0].strip()\n",
    "        end_date = date_parts[1].strip() if len(date_parts) > 1 else None\n",
    "        \n",
    "        # Check if the month text is written in *both* the date strings\n",
    "        # Required work-around as Ofsted reports contain inspection date strings in multiple formats (i/ii/iii...)\n",
    "        #   i)      \"15 to 26 November\"  \n",
    "        #   ii)     \"28 February to 4 March\" or \"8 October to 19 October\" (majority)\n",
    "        #   iii)    ['8 July ', '12 July   and 7 August  to'] (*recently seen)\n",
    "        #   iv)     \"11 September 2017 to 5 October 2017\" (double year)\n",
    "        #   v)      \"Inspection dates: 19 November–30 November 2018\" (Bromley)\n",
    "        if len(start_date) <= 2: # i.e. do we only have a date with no month text\n",
    "            inspection_month = end_date.split()[1]\n",
    "            start_date = f\"{start_date} {inspection_month}\"\n",
    "\n",
    "        # Append the inspection year to the start_date and end_date\n",
    "        start_date_full = f\"{start_date} {year}\"\n",
    "        end_date_full = f\"{end_date} {year}\" if end_date else None\n",
    "\n",
    "        # Parse the dates into datetime objects and format them as \"dd/mm/yyyy\"\n",
    "        start_date_formatted = parser.parse(start_date_full).strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        # Have removed date formatting on end date until bug of multiple end dates solved\n",
    "        #end_date_formatted = parser.parse(end_date_full).strftime(\"%d/%m/%Y\") if end_date_full else None\n",
    "\n",
    "    else:\n",
    "        start_date_formatted = None\n",
    "        end_date_full = None\n",
    "\n",
    "\n",
    "    return {'inspector_name': inspector_name, 'overall_inspection_grade': inspection_grade,'inspection_start_date': start_date_formatted,'inspection_end_date': end_date_full}\n",
    "\n",
    "\n",
    "def process_provider_links(provider_links):\n",
    "    \"\"\"\n",
    "    Processes provider links and returns a list of dictionaries containing URN, local authority, and inspection link.\n",
    "\n",
    "    Args:\n",
    "        provider_links (list): A list of BeautifulSoup Tag objects representing provider links.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing URN, local authority, inspection link, and, if enabled, additional inspection data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    global pdf_data_capture # Bool flag\n",
    "    \n",
    "    for link in provider_links:\n",
    "        # Extract the URN and provider name from the web link shown\n",
    "        urn = link['href'].rsplit('/', 1)[-1]\n",
    "        name = clean_provider_name(link.text.strip())\n",
    "\n",
    "        # Create the provider directory path\n",
    "        provider_dir = os.path.join('.', urn + '_' + name)\n",
    "\n",
    "        # Create the provider directory if it doesn't exist\n",
    "        if not os.path.exists(provider_dir):\n",
    "            os.makedirs(provider_dir)\n",
    "\n",
    "        # Get the child page content\n",
    "        child_url = 'https://reports.ofsted.gov.uk' + link['href']\n",
    "        child_soup = get_soup(child_url)\n",
    "\n",
    "        # Find all publication links in the provider's child page\n",
    "        pdf_links = child_soup.find_all('a', {'class': 'publication-link'})\n",
    "\n",
    "        # Initialize a flag to indicate if an inspection link has been found\n",
    "        # Important: This assumes that the provider's reports are returned/organised most recent FIRST\n",
    "        found_inspection_link = False\n",
    "\n",
    "        # Iterate through the publication links\n",
    "        for pdf_link in pdf_links:\n",
    "\n",
    "            # Check if the current/next href-link meets the selection criteria\n",
    "            # This block obv relies on Ofsted continued use of nonvisual element descriptors\n",
    "            # containing the type(s) of inspection text. We use  \"children's services inspection\"\n",
    "\n",
    "            nonvisual_text = pdf_link.select_one('span.nonvisual').text.lower().strip()\n",
    "\n",
    "            # For now at least, search terms hard-coded. \n",
    "            if 'children' in nonvisual_text and 'services' in nonvisual_text and 'inspection' in nonvisual_text:\n",
    "\n",
    "\n",
    "                # Create the filename and download the PDF (this filetype needs to be hard-coded here)\n",
    "                filename = nonvisual_text.replace(', pdf', '') + '.pdf'\n",
    "\n",
    "                pdf_content = requests.get(pdf_link['href']).content\n",
    "                with open(os.path.join(provider_dir, filename), 'wb') as f:\n",
    "                    f.write(pdf_content)\n",
    "\n",
    "\n",
    "               # Extract the local authority and inspection link, and add the data to the list\n",
    "                if not found_inspection_link:\n",
    "\n",
    "                    # Capture the data that will be exported about the most recent inspection only\n",
    "                    local_authority = provider_dir.split('_', 1)[-1].replace('_', ' ').strip()\n",
    "                    inspection_link = pdf_link['href']\n",
    "                    \n",
    "                    # Extract the report published date\n",
    "                    report_published_date_str = filename.split('-')[-1].strip().split('.')[0] # published date appears after '-' \n",
    "            \n",
    "                    report_published_date = datetime.datetime.strptime(report_published_date_str, '%d %B %Y') # str to datetime\n",
    "                    report_published_date = report_published_date.strftime('%d/%m/%y') # Format dd/mm/yy\n",
    " \n",
    "\n",
    "                    # Now get the in-document data\n",
    "                    if pdf_data_capture:\n",
    "                        # Opt1 : ~x4 slower runtime\n",
    "                        # Only here if we have set PDF text scrape flag to True\n",
    "                        # Turn this off, speeds up script if we only need the inspection documents themselves to be retrieved\n",
    "\n",
    "                        # Scrape inside the pdf inspection reports\n",
    "                        inspection_data_dict = extract_inspection_data(pdf_content)\n",
    "                        \n",
    "                        # Added for readability of returned data/onward\n",
    "                        overall_effectiveness = inspection_data_dict['overall_inspection_grade']\n",
    "                        inspector_name = inspection_data_dict['inspector_name']\n",
    "                        inspection_start_date = inspection_data_dict['inspection_start_date']\n",
    "                        inspection_end_date = inspection_data_dict['inspection_end_date']\n",
    "                        \n",
    "                        # Format the provider directory as a file path link for Excel\n",
    "                        provider_dir_link = 'file:///' + provider_dir.replace(\"\\\\\", \"/\")\n",
    "                                            \n",
    "                        data.append({\n",
    "                                        'urn': urn,\n",
    "                                        'local_authority': local_authority,\n",
    "                                        'inspection_link': inspection_link,\n",
    "                                        'overall_effectiveness': overall_effectiveness,\n",
    "                                        'inspector_name': inspector_name,\n",
    "                                        'inspection_start_date': inspection_start_date,\n",
    "                                        'inspection_end_date': inspection_end_date,\n",
    "                                        'publication_date': report_published_date,\n",
    "                                        'local_link_to_all_inspections': provider_dir_link\n",
    "                                    })\n",
    "                        \n",
    "                    else:\n",
    "                        # Opt2 : ~x4 faster runtime\n",
    "                        # Only grab the data/docs we can get direct off the Ofsted page \n",
    "                        data.append({'urn': urn, 'local_authority': local_authority, 'inspection_link': inspection_link})\n",
    "\n",
    "                    \n",
    "                    found_inspection_link = True # Flag to ensure data reporting on only the most recent inspection\n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_pagination(soup, url_stem):\n",
    "    \"\"\"\n",
    "    Handles pagination for a BeautifulSoup object representing a web page with paginated content.\n",
    "    \n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the web page.\n",
    "        url_stem (str): The base URL to which the relative path of the next page will be appended.\n",
    "        \n",
    "    Returns:\n",
    "        str: The full URL of the next page if it exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the pagination element in the soup object\n",
    "    pagination = soup.find('ul', {'class': 'pagination'})\n",
    "\n",
    "    # Check if the pagination element exists\n",
    "    if pagination:\n",
    "        # Find the next page button in the pagination element\n",
    "        next_page_button = pagination.find('li', {'class': 'next'})\n",
    "\n",
    "        # Check if the next page button exists\n",
    "        if next_page_button:\n",
    "            # Extract the relative URL of the next page\n",
    "            next_page_url = next_page_button.find('a')['href']\n",
    "            \n",
    "            # Return the full URL of the next page by appending the relative URL to the base URL\n",
    "            return url_stem + next_page_url\n",
    "\n",
    "    # Return None if there is no next page button or pagination element\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_data(data, filename, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Exports data to a specified file type.\n",
    "\n",
    "    Args:\n",
    "        data (list or dict): The data to be exported.\n",
    "        filename (str): The desired name of the output file.\n",
    "        file_type (str, optional): The desired file type. Defaults to 'csv'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        filename_with_extension = filename + '.csv'\n",
    "        pd.DataFrame(data).to_csv(filename_with_extension, index=False)\n",
    "\n",
    "    elif file_type == 'excel':\n",
    "        filename_with_extension = filename + '.xlsx'\n",
    "        pd.DataFrame(data).to_excel(filename_with_extension, index=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: unsupported file type '{file_type}'. Please choose 'csv' or 'xlsx'.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{filename_with_extension} successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Apr 26, 2023 9:56:04 AM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider loadDiskCache\n",
      "WARNING: New fonts found, font cache will be re-built\n",
      "Apr 26, 2023 9:56:04 AM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
      "WARNING: Building on-disk font cache, this may take a while\n",
      "Apr 26, 2023 9:56:04 AM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
      "WARNING: Finished building on-disk font cache, found 6 fonts\n",
      "Apr 26, 2023 9:56:04 AM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "Apr 26, 2023 9:56:05 AM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "\n",
      "Got stderr: Apr 26, 2023 9:56:21 AM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n",
      "Got stderr: Apr 26, 2023 9:56:56 AM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'export_file_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Export summary data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m save_data(data, export_summary_filename, export_file_type)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'export_file_type' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scrape data\n",
    "\n",
    "data = []\n",
    "while True:\n",
    "    # Fetch and parse the HTML content of the current URL\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # Find all 'provider' links on the page\n",
    "    provider_links = soup.find_all('a', href=lambda href: href and '/provider/' in href)\n",
    "\n",
    "    # Process the provider links and extend the data list with the results\n",
    "    data.extend(process_provider_links(provider_links))\n",
    "\n",
    "    \n",
    "    # Since all results are on a single page, no need to handle pagination. \n",
    "    # Processing complete.   \n",
    "    break\n",
    "\n",
    "\n",
    "# Export summary data\n",
    "save_data(data, export_summary_filename, export_file_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
