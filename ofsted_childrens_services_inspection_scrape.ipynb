{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ofsted Inspection Reports Scrape Tool \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export options\n",
    "\n",
    "export_summary_filename = 'ofsted_tracker_update'\n",
    "export_filetype         = 'csv' # Excel / csv currently supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Script admin settings\n",
    "\n",
    "# Keep warnings quiet unless priority\n",
    "logging.getLogger('org.apache.pdfbox').setLevel(logging.ERROR)\n",
    "\n",
    "pdf_data_capture = True # Set to True to also scrape inspection grade/data from pdf reports into csv\n",
    "                        # This impacts run time quite heavily E.g False ~1m20 / True ~ 4m10\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-standard modules that might need installing\n",
    "# !pip install PyPDF2\n",
    "# !pip install tabula-py\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import logging\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# pdf search/data extraction\n",
    "import io\n",
    "import tabula   \n",
    "import PyPDF2   \n",
    "import re       \n",
    "\n",
    "# used in handling inspection dates\n",
    "from dateutil import parser \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function defs\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given a URL, returns a BeautifulSoup object.\n",
    "    Args: url (str): The URL to fetch and parse.\n",
    "    Returns: BeautifulSoup: The parsed HTML content.\n",
    "    \"\"\"\n",
    "    timeout_seconds = 10 # lets not assume the Ofsted page is up\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout_seconds)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except RequestException as e:\n",
    "        print(f\"An error occurred while fetching the URL '{url}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_provider_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the name according to the logic provided.\n",
    "    Args:\n",
    "        name (str): The original name to be cleaned.\n",
    "    Returns:\n",
    "        str: The cleaned name.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    name = name.lower().replace('  ', ' ')\n",
    "    \n",
    "    # Remove specific phrases\n",
    "    name = name.replace(\"city of \", \"\").replace(\"metropolitan district council\", \"\").replace(\"london borough of\", \"\").replace(\"council of\", \"\")\n",
    "    \n",
    "    # Remove undesired words and join the remaining parts\n",
    "    name_parts = [part for part in name.split() if part not in ['city', 'metropolitan', 'borough', 'council', 'county', 'district', 'the']]\n",
    "    return ' '.join(name_parts)\n",
    "\n",
    "\n",
    "\n",
    "def extract_inspection_data(pdf_content):\n",
    "    \"\"\"\n",
    "    Extracts the inspector's name, overall Ofsted grade, and inspection dates from the first page of a PDF report.\n",
    "\n",
    "    Args:\n",
    "        pdf_content (bytes): The content of the PDF file as bytes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the inspector's name, overall Ofsted grade, and inspection dates, or None if not found.\n",
    "\n",
    "    Notes:\n",
    "        This function extracts information from the first page of the PDF report. The inspector's name is extracted using a\n",
    "        regular expression search for the string \"Lead inspector:\". The overall Ofsted grade is extracted from a table that\n",
    "        appears on the first page of the report. The function uses the tabula library to extract the table data. The inspection\n",
    "        dates are also extracted using a regular expression search for the string \"Inspection dates:\". The function attempts to\n",
    "        parse the inspection dates into datetime objects and format them as \"dd/mm/yyyy\". The final output is a dictionary\n",
    "        containing the extracted information or None if any of the information could not be found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when an unknown grade value is found during grade extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a file-like buffer for the PDF content\n",
    "    with io.BytesIO(pdf_content) as buffer:\n",
    "        # Read the PDF content for text extraction\n",
    "        reader = PyPDF2.PdfReader(buffer)\n",
    "        page = reader.pages[0]\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Find the inspector's name using a regular expression\n",
    "        match = re.search(r\"Lead inspector:\\s*(.+)\", text)\n",
    "        if match:\n",
    "            inspector_name = match.group(1)\n",
    "            \n",
    "            inspector_name = inspector_name.split(',')[0].strip()       # Remove everything after the first comma\n",
    "            inspector_name = inspector_name.replace(\"HMI\", \"\").rstrip() # Remove \"HMI\" and any trailing spaces\n",
    "\n",
    "        else:\n",
    "            inspector_name = None\n",
    "\n",
    "        # Read the PDF and extract the table on the first page\n",
    "        try:\n",
    "            buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "            tables = tabula.read_pdf(buffer, pages=1, multiple_tables=True)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the PDF: {e}\")\n",
    "            tables = []\n",
    "\n",
    "    # Find the 'Overall effectiveness' row and extract the corresponding grade\n",
    "    inspection_grade = None\n",
    "    for table in tables:\n",
    "        if 'Judgement' in table.columns and 'Grade' in table.columns:\n",
    "            for index, row in table.iterrows():\n",
    "                if row['Judgement'] == 'Overall effectiveness':\n",
    "\n",
    "                    try:\n",
    "                        inspection_grade = str(row['Grade']) # Convert the grade to a string (previously float values found in some reports)\n",
    "\n",
    "                        if \"requires improvement\" in inspection_grade.lower():\n",
    "                            # Some RI texts come with extra wording/baggage. Overwrite with clean val to aid clean-up\n",
    "                            inspection_grade = \"Requires improvement\"\n",
    "\n",
    "                    except Exception as e:\n",
    "                        inspection_grade = \"Unknown value type : \" + inspection_grade\n",
    "\n",
    "                        error_msg = f\"unknown value found: \\\"unknown : {inspection_grade}\\\"\"\n",
    "                        raise ValueError(error_msg)\n",
    "                        \n",
    "                    break\n",
    "                \n",
    "            if inspection_grade is not None:\n",
    "                break\n",
    "\n",
    "\n",
    "    # Find the inspection dates using a regular expression\n",
    "    date_match = re.search(r\"Inspection dates:\\s*(.+)\", text)\n",
    "\n",
    "    if date_match:\n",
    "        # IF there was date data\n",
    "\n",
    "\n",
    "        inspection_dates = date_match.group(1).strip()\n",
    "            \n",
    "        # Some initial clean up based on historic data obs\n",
    "        inspection_dates = inspection_dates.replace(\".\", \"\")\n",
    "        inspection_dates = inspection_dates.replace(\"\\u00A0\", \" \") # Remove non-breaking space (Seen in nottingham report)\n",
    "        inspection_dates = re.sub(r\"[\\u2012\\u2013\\u2014\\u2212\\-]+\", \" to \", inspection_dates) # replace en dash char (\"\\u2013\"), em dash (\"\\u2014\"), or (\"-\") \n",
    "        inspection_dates = inspection_dates.split(\"and\")[0].strip() # Need this because we have such as :\n",
    "                                                                    # \"8 July 2019 to 12 July 2019 and 7 August 2019 to 8 August 2019\"\n",
    "                                                                    # E.g. Derbyshire\n",
    "        inspection_dates = re.sub(r'(\\d)\\s(\\d)', r'\\1\\2', inspection_dates) # Fix white spaces between date numbers e.g. \"wiltshire,\t1 9 June 2019\"\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(inspection_dates, str):\n",
    "            # data was as expected\n",
    "            year_match = re.search(r\"\\d{4}\", inspection_dates)\n",
    "            if year_match:\n",
    "                year = year_match.group(0) # get single copy of yyyy\n",
    "\n",
    "                # Now remove the year from the inspection_dates string\n",
    "                inspection_dates_cleaned = inspection_dates.replace(year, \"\").strip()\n",
    "\n",
    "            else:\n",
    "                # We had inspection_dates data but no recognisable year\n",
    "                year = None\n",
    "                inspection_dates_cleaned = inspection_dates\n",
    "\n",
    "        else:\n",
    "            # spurious data\n",
    "            # inspection_dates arrived with non-str, set default val\n",
    "            print(\"Error: inspection_dates is not a string. Type is\", type(inspection_dates))\n",
    "            inspection_dates_cleaned = None \n",
    "\n",
    "\n",
    "        # Now that we have already removed/cleaned those with 'and .....'\n",
    "        # Split the inspection_dates_cleaned string using ' to ' as the delimiter and limit the number of splits to 1\n",
    "        date_parts = inspection_dates_cleaned.split(' to ', maxsplit=1) # expect only 1 instance of 'to' between date vals\n",
    "        \n",
    "\n",
    "  \n",
    "        # Get the seperate inspection date(s) \n",
    "        start_date = date_parts[0].strip()\n",
    "        end_date = date_parts[1].strip() if len(date_parts) > 1 else None\n",
    "        \n",
    "        # Check if the month text is written in *both* the date strings\n",
    "        # Required work-around as Ofsted reports contain inspection date strings in multiple formats (i/ii/iii...)\n",
    "        #   i)      \"15 to 26 November\"  \n",
    "        #   ii)     \"28 February to 4 March\" or \"8 October to 19 October\" (majority)\n",
    "        #   iii)    ['8 July ', '12 July   and 7 August  to'] (*recently seen)\n",
    "        #   iv)     \"11 September 2017 to 5 October 2017\" (double year)\n",
    "        #   v)      \"Inspection dates: 19 Novemberâ€“30 November 2018\" (Bromley)\n",
    "        if len(start_date) <= 2: # i.e. do we only have a date with no month text\n",
    "            inspection_month = end_date.split()[1]\n",
    "            start_date = f\"{start_date} {inspection_month}\"\n",
    "\n",
    "        # Append the inspection year to the start_date and end_date\n",
    "        start_date_full = f\"{start_date} {year}\"\n",
    "        end_date_full = f\"{end_date} {year}\" if end_date else None\n",
    "\n",
    "        # Parse the dates into datetime objects and format them as \"dd/mm/yyyy\"\n",
    "        start_date_formatted = parser.parse(start_date_full).strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        # Have removed date formatting on end date until bug of multiple end dates solved\n",
    "        #end_date_formatted = parser.parse(end_date_full).strftime(\"%d/%m/%Y\") if end_date_full else None\n",
    "\n",
    "    else:\n",
    "        start_date_formatted = None\n",
    "        end_date_full = None\n",
    "\n",
    "\n",
    "    return {'inspector_name': inspector_name, 'overall_inspection_grade': inspection_grade,'inspection_start_date': start_date_formatted,'inspection_end_date': end_date_full}\n",
    "\n",
    "\n",
    "def process_provider_links(provider_links):\n",
    "    \"\"\"\n",
    "    Processes provider links and returns a list of dictionaries containing URN, local authority, and inspection link.\n",
    "\n",
    "    Args:\n",
    "        provider_links (list): A list of BeautifulSoup Tag objects representing provider links.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing URN, local authority, inspection link, and, if enabled, additional inspection data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    global pdf_data_capture # Bool flag\n",
    "    \n",
    "    for link in provider_links:\n",
    "        # Extract the URN and provider name from the web link shown\n",
    "        urn = link['href'].rsplit('/', 1)[-1]\n",
    "        name = clean_provider_name(link.text.strip())\n",
    "\n",
    "        # Create the provider directory path\n",
    "        provider_dir = os.path.join('.', urn + '_' + name)\n",
    "\n",
    "        # Create the provider directory if it doesn't exist\n",
    "        if not os.path.exists(provider_dir):\n",
    "            os.makedirs(provider_dir)\n",
    "\n",
    "        # Get the child page content\n",
    "        child_url = 'https://reports.ofsted.gov.uk' + link['href']\n",
    "        child_soup = get_soup(child_url)\n",
    "\n",
    "        # Find all publication links in the provider's child page\n",
    "        pdf_links = child_soup.find_all('a', {'class': 'publication-link'})\n",
    "\n",
    "        # Initialize a flag to indicate if an inspection link has been found\n",
    "        # Important: This assumes that the provider's reports are returned/organised most recent FIRST\n",
    "        found_inspection_link = False\n",
    "\n",
    "        # Iterate through the publication links\n",
    "        for pdf_link in pdf_links:\n",
    "\n",
    "            # Check if the current/next href-link meets the selection criteria\n",
    "            # This block obv relies on Ofsted continued use of nonvisual element descriptors\n",
    "            # containing the type(s) of inspection text. We use  \"children's services inspection\"\n",
    "\n",
    "            nonvisual_text = pdf_link.select_one('span.nonvisual').text.lower().strip()\n",
    "            if 'children' in nonvisual_text and 'services' in nonvisual_text and 'inspection' in nonvisual_text:\n",
    "\n",
    "\n",
    "                # Create the filename and download the PDF\n",
    "                filename = nonvisual_text.replace(', pdf', '') + '.pdf'\n",
    "\n",
    "                pdf_content = requests.get(pdf_link['href']).content\n",
    "                with open(os.path.join(provider_dir, filename), 'wb') as f:\n",
    "                    f.write(pdf_content)\n",
    "\n",
    "\n",
    "               # Extract the local authority and inspection link, and add the data to the list\n",
    "                if not found_inspection_link:\n",
    "\n",
    "                    # Capture the data that will be exported about the most recent inspection only\n",
    "                    local_authority = provider_dir.split('_', 1)[-1].replace('_', ' ').strip()\n",
    "                    inspection_link = pdf_link['href']\n",
    "                    \n",
    "                    # Extract the report published date\n",
    "                    report_published_date_str = filename.split('-')[-1].strip().split('.')[0] # published date appears after '-' \n",
    "            \n",
    "                    report_published_date = datetime.datetime.strptime(report_published_date_str, '%d %B %Y') # str to datetime\n",
    "                    report_published_date = report_published_date.strftime('%d/%m/%y') # Format dd/mm/yy\n",
    " \n",
    "\n",
    "                    # Now get the in-document data\n",
    "                    if pdf_data_capture:\n",
    "                        # Opt1 : ~x4 slower runtime\n",
    "                        # Only here if we have set PDF text scrape flag to True\n",
    "                        # Turn this off, speeds up script if we only need the inspection documents themselves to be retrieved\n",
    "\n",
    "                        # Scrape inside the pdf inspection reports\n",
    "                        inspection_data_dict = extract_inspection_data(pdf_content)\n",
    "                        \n",
    "                        # Added for readability of returned data/onward\n",
    "                        overall_effectiveness = inspection_data_dict['overall_inspection_grade']\n",
    "                        inspector_name = inspection_data_dict['inspector_name']\n",
    "                        inspection_start_date = inspection_data_dict['inspection_start_date']\n",
    "                        inspection_end_date = inspection_data_dict['inspection_end_date']\n",
    "                        \n",
    "                        # Format the provider directory as a file path link for Excel\n",
    "                        provider_dir_link = 'file:///' + provider_dir.replace(\"\\\\\", \"/\")\n",
    "                                            \n",
    "                        data.append({\n",
    "                                        'urn': urn,\n",
    "                                        'local_authority': local_authority,\n",
    "                                        'inspection_link': inspection_link,\n",
    "                                        'overall_effectiveness': overall_effectiveness,\n",
    "                                        'inspector_name': inspector_name,\n",
    "                                        'inspection_start_date': inspection_start_date,\n",
    "                                        'inspection_end_date': inspection_end_date,\n",
    "                                        'publication_date': report_published_date,\n",
    "                                        'local_link_to_all_inspections': provider_dir_link\n",
    "                                    })\n",
    "                        \n",
    "                    else:\n",
    "                        # Opt2 : ~x4 faster runtime\n",
    "                        # Only grab the data/docs we can get direct off the Ofsted page \n",
    "                        data.append({'urn': urn, 'local_authority': local_authority, 'inspection_link': inspection_link})\n",
    "\n",
    "                    \n",
    "                    found_inspection_link = True # Flag to ensure data reporting on only the most recent inspection\n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_pagination(soup, url_stem):\n",
    "    \"\"\"\n",
    "    Handles pagination for a BeautifulSoup object representing a web page with paginated content.\n",
    "    \n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the web page.\n",
    "        url_stem (str): The base URL to which the relative path of the next page will be appended.\n",
    "        \n",
    "    Returns:\n",
    "        str: The full URL of the next page if it exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the pagination element in the soup object\n",
    "    pagination = soup.find('ul', {'class': 'pagination'})\n",
    "\n",
    "    # Check if the pagination element exists\n",
    "    if pagination:\n",
    "        # Find the next page button in the pagination element\n",
    "        next_page_button = pagination.find('li', {'class': 'next'})\n",
    "\n",
    "        # Check if the next page button exists\n",
    "        if next_page_button:\n",
    "            # Extract the relative URL of the next page\n",
    "            next_page_url = next_page_button.find('a')['href']\n",
    "            \n",
    "            # Return the full URL of the next page by appending the relative URL to the base URL\n",
    "            return url_stem + next_page_url\n",
    "\n",
    "    # Return None if there is no next page button or pagination element\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_data(data, filename, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Exports data to a specified file type.\n",
    "\n",
    "    Args:\n",
    "        data (list or dict): The data to be exported.\n",
    "        filename (str): The desired name of the output file.\n",
    "        file_type (str, optional): The desired file type. Defaults to 'csv'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        filename_with_extension = filename + '.csv'\n",
    "        pd.DataFrame(data).to_csv(filename_with_extension, index=False)\n",
    "\n",
    "    elif file_type == 'excel':\n",
    "        filename_with_extension = filename + '.xlsx'\n",
    "        pd.DataFrame(data).to_excel(filename_with_extension, index=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: unsupported file type '{file_type}'. Please choose 'csv' or 'xlsx'.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{filename_with_extension} successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ofsted site/page admin settings\n",
    "\n",
    "max_page_results = 200 # Set max number of search results to show on page(MUST be > total number of LA's!) \n",
    "url_stem = 'https://reports.ofsted.gov.uk/'\n",
    "search_url = 'search?q=&location=&lat=&lon=&radius=&level_1_types=3&level_2_types%5B%5D=12' # Can improve this\n",
    "max_page_results_url = '&rows=' + str(max_page_results) # Coerce results page to display ALL providers on single without next/pagination\n",
    "url = url_stem + search_url + max_page_results_url\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Apr 25, 2023 11:59:00 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "Apr 25, 2023 11:59:01 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "\n",
      "Got stderr: Apr 25, 2023 11:59:15 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n",
      "Got stderr: Apr 25, 2023 11:59:46 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scrape data\n",
    "\n",
    "data = []\n",
    "while True:\n",
    "    # Fetch and parse the HTML content of the current URL\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # Find all 'provider' links on the page\n",
    "    provider_links = soup.find_all('a', href=lambda href: href and '/provider/' in href)\n",
    "\n",
    "    # Process the provider links and extend the data list with the results\n",
    "    data.extend(process_provider_links(provider_links))\n",
    "\n",
    "    \n",
    "    # Since all results are on a single page, no need to handle pagination. \n",
    "    # Processing complete.   \n",
    "    break\n",
    "\n",
    "\n",
    "# Export summary data\n",
    "save_data(data, export_summary_filename, export_file_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
